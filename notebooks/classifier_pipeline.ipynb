{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07109d7",
   "metadata": {},
   "source": [
    "# Feedback Classifier â€“ Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef7c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"../data/dummy_feedback.csv\")\n",
    "\n",
    "# Clean text\n",
    "df['feedback'] = df['feedback'].str.lower().str.strip()\n",
    "df['topics'] = df['topics'].str.lower().str.strip()\n",
    "df['topics'] = df['topics'].apply(lambda x: [t.strip() for t in x.split(',')])\n",
    "\n",
    "# Multi-label binarisation\n",
    "mlb = MultiLabelBinarizer()\n",
    "topic_labels = mlb.fit_transform(df['topics'])\n",
    "\n",
    "# TF-IDF vectorisation\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(df['feedback'])\n",
    "\n",
    "# Prepare targets\n",
    "y_sentiment = df['sentiment']\n",
    "y_combined = np.hstack([y_sentiment.values.reshape(-1, 1), topic_labels])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = MultiOutputClassifier(LogisticRegression(max_iter=1000))\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "y_test_sentiment, y_test_topics = y_test[:, 0], y_test[:, 1:]\n",
    "y_pred_sentiment, y_pred_topics = y_pred[:, 0], y_pred[:, 1:]\n",
    "\n",
    "# Evaluate\n",
    "print(\"Sentiment Accuracy:\", accuracy_score(y_test_sentiment, y_pred_sentiment))\n",
    "print(\"Topic Precision:\", precision_score(y_test_topics, y_pred_topics, average='micro'))\n",
    "print(\"Topic Recall:\", recall_score(y_test_topics, y_pred_topics, average='micro'))\n",
    "print(\"Topic F1 Score:\", f1_score(y_test_topics, y_pred_topics, average='micro'))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}